[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "30/30 AI PAPER LIST - NOTES",
    "section": "",
    "text": "1993 - Keeping Neural Networks Simple By Minimizing the Description Length of the Weights 2004 - A Tutorial Introduction to the Minimum Description Length Principle 2008 - Machine Super Intelligence 2011 - The First Law of Complexodynamics 2012 - ImageNet Classification with Deep Convolutional Neural Networks 2014 - Neural Turing Machines 2014 - Quantifying the Rise and Fall of Complexity in Closed Systems 2015 - Deep Residual Learning for Image Recognition 2015 - Neural Machine Translation by Jointly Learning to Align and Translate 2015 - Pointer Networks 2015 - Recurrent Neural Network Regularization 2015 - The Unreasonable Effectiveness of Recurrent Neural Networks 2015 - Understanding LSTM Networks 2016 - Deep Speech 2: End-to-End Speech Recognition in English and Mandarin 2016 - Identity Mappings in Deep Residual Networks 2016 - Multi-Scale Context Aggregation by Dilated Convolutions 2016 - Order Matters: Sequence to sequence for sets 2016 - Variational Lossy Autoencoder 2017 - A Simple Neural Network Module for Relational Reasoning 2017 - Attention is All You Need 2017 - Kolmogorov Complexity and Algorithmic Randomness 2017 - Neural Message Passing for Quantum Chemistry 2018 - Relational Recurrent Neural Networks 2019 - GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism 2020 - Scaling Laws for Neural Language Models 2024 - CS231n Convolutional Neural Networks for Visual Recognition"
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "MDLP",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall MDLP in Development mode\n# make sure MDLP package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to MDLP\n$ nbdev_prepare"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "MDLP",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/fevzikilas/MDLP.git\nor from conda\n$ conda install -c fevzikilas MDLP\nor from pypi\n$ pip install MDLP\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "MDLP",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "MDLP NOTES",
    "section": "",
    "text": "1- Keeping Neural Networks Simple By Minimizing the Description Length of the Weights\nMinimum Description Length\nKeeping Neural Networks Simple by Minimizing the Description Length of the Weights by Geofrey E. Hinton and Drew van Camp\nŞimdi, ilk makale ile başlıyorum. 1993 Minimum Description Length Principle ile iligli.\nEğer eğitim sırasında weightlerde, output vectorlerinden daha az bilgi varsa network daha iyi genelleme yapıyormuş. Yani eğitim sırasında weightleri basit tutmak baya önemliymiş. Bunu yapmak işçin de weightin içerdiği biligi miktarını cezalandırabiliriz. Bunu kontrol etmek için Gaussian noise eklenebiliriz böylece bu noise leveller, ağın expected squared error’u ile weight’lerdeki bigi miktarı arasındaki dengeyi optimize etmek için öğrenme sırasında kullanılabiilr.\nNon-linear hidden birimler layerı içeren bir networkün beklenen squared error’un ve noisy weightslerde bulunan bilgi miktarının türevlerini kullanaran bir yöntemini açıklımışlar.\nOutputlar doğrusal olduğu takdirde, zaman alıcı Monte Carlo simülasyonlarına gerek kalmadan kesin türevler verimli bir şekilde hesaplanabilir.\nBu neural networklerdeki weightlerin iletilmesi için gereken bilgi miktarını azaltma fikri encoding the weights için bir dizi ilginç şemaya yol açarmış.\nYani burdan çıkarılacak sonuç: bu MDL prensibi, bazı dataların en iyi modelleri modelin description lengthi ile o model kullanılarak encode edilen verilerin uzunluğunun toplamını en aza indiren modeldir.\nEğer model complexityi control altına almak ve overfittingi önlemek istiyorsak, MDL prensibini kullanabiliriz.\nDescription length of the weightsi en aza indirmek için weight-sharing kullanıyor. Yani aynı weightleri birden fazla yerde kullanıyoruz. Bu da modelin complexityini azaltıyor.\n—-\n\n\n2- A Tutorial Introduction to the Minimum Description Length Principle\nİkinci makale 2004 - A Tutorial Introduction to the Minimum Description Length Principle Bu aslında bir makale değil 80 sayfalık bir açıklama dökümanı. MDL prensibini detaylı bir şekilde anlamak için bu dökümanı okumak gerekiyor.\n—-",
    "crumbs": [
      "MDLP NOTES"
    ]
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "MDLP",
    "section": "",
    "text": "1- Keeping Neural Networks Simple By Minimizing the Description Length of the Weights\nMinimum Description Length\nKeeping Neural Networks Simple by Minimizing the Description Length of the Weights by Geofrey E. Hinton and Drew van Camp\nŞimdi, ilk makale ile başlıyorum. 1993 Minimum Description Length Principle ile iligli.\nEğer eğitim sırasında weightlerde, output vectorlerinden daha az bilgi varsa network daha iyi genelleme yapıyormuş. Yani eğitim sırasında weightleri basit tutmak baya önemliymiş. Bunu yapmak işçin de weightin içerdiği biligi miktarını cezalandırabiliriz. Bunu kontrol etmek için Gaussian noise eklenebiliriz böylece bu noise leveller, ağın expected squared error’u ile weight’lerdeki bigi miktarı arasındaki dengeyi optimize etmek için öğrenme sırasında kullanılabiilr.\nNon-linear hidden birimler layerı içeren bir networkün beklenen squared error’un ve noisy weightslerde bulunan bilgi miktarının türevlerini kullanaran bir yöntemini açıklımışlar.\nOutputlar doğrusal olduğu takdirde, zaman alıcı Monte Carlo simülasyonlarına gerek kalmadan kesin türevler verimli bir şekilde hesaplanabilir.\nBu neural networklerdeki weightlerin iletilmesi için gereken bilgi miktarını azaltma fikri encoding the weights için bir dizi ilginç şemaya yol açarmış.\nYani burdan çıkarılacak sonuç: bu MDL prensibi, bazı dataların en iyi modelleri modelin description lengthi ile o model kullanılarak encode edilen verilerin uzunluğunun toplamını en aza indiren modeldir.\nEğer model complexityi control altına almak ve overfittingi önlemek istiyorsak, MDL prensibini kullanabiliriz.\nDescription length of the weightsi en aza indirmek için weight-sharing kullanıyor. Yani aynı weightleri birden fazla yerde kullanıyoruz. Bu da modelin complexityini azaltıyor.\n—-\n\n\n2- A Tutorial Introduction to the Minimum Description Length Principle\nİkinci makale 2004 - A Tutorial Introduction to the Minimum Description Length Principle Bu aslında bir makale değil 80 sayfalık bir açıklama dökümanı. MDL prensibini detaylı bir şekilde anlamak için bu dökümanı okumak gerekiyor.\n—-",
    "crumbs": [
      "1- Keeping Neural Networks Simple By Minimizing the Description Length of the Weights"
    ]
  }
]
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 30/30 AI PAPERS NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1993 - Keeping Neural Networks Simple By Minimizing the Description Length of the Weights<br>\n",
    "2004 - A Tutorial Introduction to the Minimum Description Length Principle<br>\n",
    "2008 - Machine Super Intelligence<br>\n",
    "2011 - The First Law of Complexodynamics<br>\n",
    "2012 - ImageNet Classification with Deep Convolutional Neural Networks<br>\n",
    "2014 - Neural Turing Machines<br>\n",
    "2014 - Quantifying the Rise and Fall of Complexity in Closed Systems<br>\n",
    "2015 - Deep Residual Learning for Image Recognition<br>\n",
    "2015 - Neural Machine Translation by Jointly Learning to Align and Translate<br>\n",
    "2015 - Pointer Networks<br>\n",
    "2015 - Recurrent Neural Network Regularization<br>\n",
    "2015 - The Unreasonable Effectiveness of Recurrent Neural Networks<br>\n",
    "2015 - Understanding LSTM Networks<br>\n",
    "2016 - Deep Speech 2: End-to-End Speech Recognition in English and Mandarin<br>\n",
    "2016 - Identity Mappings in Deep Residual Networks<br>\n",
    "2016 - Multi-Scale Context Aggregation by Dilated Convolutions<br>\n",
    "2016 - Order Matters: Sequence to sequence for sets<br>\n",
    "2016 - Variational Lossy Autoencoder<br>\n",
    "2017 - A Simple Neural Network Module for Relational Reasoning<br>\n",
    "2017 - Attention is All You Need<br>\n",
    "2017 - Kolmogorov Complexity and Algorithmic Randomness<br>\n",
    "2017 - Neural Message Passing for Quantum Chemistry<br>\n",
    "2018 - Relational Recurrent Neural Networks<br>\n",
    "2019 - GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism<br>\n",
    "2020 - Scaling Laws for Neural Language Models<br>\n",
    "2024 - CS231n Convolutional Neural Networks for Visual Recognition<br>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Neural Networks Simple By Minimizing the Description Length of the Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Minimum Description Length](https://en.wikipedia.org/wiki/Minimum_description_length)\n",
    "\n",
    "[Keeping Neural Networks Simple by Minimizing the Description Length of the Weights by Geofrey E. Hinton and Drew van Camp](https://www.cs.toronto.edu/~fritz/absps/colt93.pdf)\n",
    "\n",
    "\n",
    "Şimdi, ilk makale ile başlıyorum. 1993 Minimum Description Length Principle ile iligli.\n",
    "\n",
    "Eğer eğitim sırasında weightlerde, output vectorlerinden daha az bilgi varsa network daha iyi genelleme yapıyormuş. Yani eğitim sırasında weightleri basit tutmak baya önemliymiş. Bunu yapmak işçin de weightin içerdiği biligi miktarını cezalandırabiliriz. Bunu kontrol etmek için Gaussian noise eklenebiliriz böylece bu noise leveller, ağın expected squared error'u ile weight'lerdeki bigi miktarı arasındaki dengeyi optimize etmek için öğrenme sırasında kullanılabiilr.\n",
    "\n",
    "Non-linear hidden birimler layerı içeren bir networkün beklenen squared error'un ve noisy weightslerde bulunan bilgi miktarının türevlerini kullanaran bir yöntemini açıklımışlar.\n",
    "\n",
    "Outputlar doğrusal olduğu takdirde, zaman alıcı Monte Carlo simülasyonlarına gerek kalmadan kesin türevler verimli bir şekilde hesaplanabilir. \n",
    "\n",
    "Bu neural networklerdeki weightlerin iletilmesi için gereken bilgi miktarını azaltma fikri encoding the weights için bir dizi ilginç şemaya yol açarmış. \n",
    "\n",
    "Yani burdan çıkarılacak sonuç: bu MDL prensibi, bazı dataların en iyi modelleri modelin description lengthi ile o model kullanılarak encode edilen verilerin uzunluğunun toplamını en aza indiren modeldir.\n",
    "\n",
    "Eğer model complexityi control altına almak ve overfittingi önlemek istiyorsak, MDL prensibini kullanabiliriz.\n",
    "\n",
    "Description length of the weightsi en aza indirmek için weight-sharing kullanıyor. Yani aynı weightleri birden fazla yerde kullanıyoruz. Bu da modelin complexityini azaltıyor.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İkinci makale 2004 - A Tutorial Introduction to the Minimum Description Length Principle\n",
    "Bu aslında bir makale değil 80 sayfalık bir açıklama dökümanı. MDL prensibini detaylı bir şekilde anlamak için bu dökümanı okumak gerekiyor.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
